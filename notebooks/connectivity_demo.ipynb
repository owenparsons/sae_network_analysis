{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature connectivity demo\n",
    "\n",
    "This notebook gives a simple introduction to using the functionality of the [SAE network analysis package](https://github.com/owenparsons/sae_network_analysis). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by importing the package as well as the torch library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sae_network_analysis\n",
    "import torch\n",
    "torch.set_grad_enabled(False)  # Disable gradient computation for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if MPS (Metal Performance Shaders) is available on macOS, otherwise fall back to CUDA or CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can load the pre-trained model using HookedSAETransformer. \n",
    "This example will use the \"gpt2-small\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE, HookedSAETransformer\n",
    "model = HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the SAE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",  # The specific release of the model\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",  # The specific layer to examine (SAE id)\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load in out dataset, which is a list of athletes and the sport they play.\n",
    "This will be converted to the task dataset that we'll be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_network_analysis.data_processing import create_prompt_dataframe, process_target_tokens\n",
    "url = 'https://raw.githubusercontent.com/ali-ce/datasets/master/Most-paid-athletes/Athletes.csv'\n",
    "context_col = \"Name\"\n",
    "target_col = \"Sport\"\n",
    "prompt = \"Fact: {context_word} plays the sport of\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame with the prompts based on the athletes' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_df = create_prompt_dataframe(url, context_col, target_col, prompt)\n",
    "prompt_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the DataFrame using the model loaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_df = process_target_tokens(prompt_df, model)\n",
    "tokenised_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run feature attribution using the tokenized data and the model. The results are merged into a comparison DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_network_analysis.sae_utils import run_feature_attribution, metric_fn\n",
    "from sae_network_analysis.data_processing import convert_feature_attribution_dict_to_long, merge_dataframes\n",
    "\n",
    "merge_method = 'intersection'\n",
    "agg_method = 'last'\n",
    "attribution_dict = run_feature_attribution(tokenised_df, \"input\", model, sae, metric_fn)\n",
    "long_dict = convert_feature_attribution_dict_to_long(attribution_dict, sae)\n",
    "comparison_df = merge_dataframes(long_dict, merge_method=merge_method, agg_method=agg_method)\n",
    "comparison_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's cluster the features based on their correlation using network analysis\n",
    "We create a correlation matrix, then extract the clusters of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_network_analysis.network_analysis import cluster_features_with_network_analysis\n",
    "full_network_filename = None # Save filename for the full network (set to None to avoid saving).\n",
    "\n",
    "corr_matrix = comparison_df.corr()\n",
    "clusters = cluster_features_with_network_analysis(corr_matrix, full_network_filename, threshold=0.7, plot_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort the correlation matrix by clusters and then plot heatmaps to visualize the correlation matrix and cluster organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_network_analysis.plot_utils import plot_heatmap_single\n",
    "\n",
    "sorted_indices = []\n",
    "for cluster, indices in clusters.items():\n",
    "    sorted_indices.extend(indices)\n",
    "\n",
    "sorted_corr_matrix = corr_matrix.loc[sorted_indices, sorted_indices]\n",
    "\n",
    "corr_filename = None\n",
    "sorted_corr_filename = None\n",
    "\n",
    "plot_heatmap_single(corr_matrix, corr_filename, title='Feature Correlation Heatmap', cbar_range=(-1,1), figsize=(9, 7))\n",
    "plot_heatmap_single(sorted_corr_matrix, sorted_corr_filename, title='Feature Correlation Heatmap (Sorted by Clusters)', cbar_range=(-1,1), figsize=(9, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filtering cluster based on size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_network_analysis.data_processing import filter_for_cluster_size_threshold, filter_for_specific_clusters\n",
    "from sae_network_analysis.plot_utils import plot_cluster_heatmap_single\n",
    "\n",
    "main_clusters = filter_for_cluster_size_threshold(clusters, threshold=10)\n",
    "subset_corr_matrix = filter_for_specific_clusters(corr_matrix, main_clusters)\n",
    "\n",
    "subset_corr_filename = None\n",
    "subset_sorted_corr_filename = None\n",
    "\n",
    "secondary_clusters = cluster_features_with_network_analysis(subset_corr_matrix, subset_corr_filename, threshold=0.7, plot_graph=True, show_edge_weights=True, show_legend=True) # This will recluster based on the subset features\n",
    "plot_cluster_heatmap_single(subset_corr_matrix, secondary_clusters, subset_sorted_corr_filename, title='Feature Correlation Heatmap Grouped by Clusters', cbar_range=(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's calculate centrality measures for the features based on the network and also include metrics for the attribution scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_network_analysis.network_analysis import compute_centrality_measures, calculate_activation_metrics\n",
    "\n",
    "centrality_df = compute_centrality_measures(corr_matrix, threshold=0.7)\n",
    "centrality_df = calculate_activation_metrics(centrality_df, comparison_df)\n",
    "\n",
    "centrality_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlations between metrics\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.heatmap(\n",
    "    centrality_df.corr(),\n",
    "    cmap=\"YlGnBu\",\n",
    "    center=0,\n",
    "    vmin=0,  # Set minimum value for colorbar\n",
    "    vmax=1,  # Set maximum value for colorbar\n",
    "    cbar_kws={'label': 'Correlation Coefficient', 'orientation': 'vertical'}  # Add colorbar label\n",
    ")\n",
    "plt.title(\"Correlation Coefficients Between Metric Scores\", fontsize=14, pad=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create an activation store to manage activations from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import ActivationsStore\n",
    "\n",
    "activation_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=8,\n",
    "    train_batch_size_tokens=4096,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the maximum activations for the selected features over multiple batches, this can be used to scale activations during feature steering if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_network_analysis.sae_utils import find_max_activations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "multi_features = torch.tensor(subset_corr_matrix.columns.values)\n",
    "max_activations = find_max_activations(model, sae, activation_store, multi_features, num_batches=100)\n",
    "\n",
    "max_act_df = pd.DataFrame(np.array([multi_features.detach().cpu().numpy(), max_activations.detach().cpu().numpy()]).T, columns=['Features', 'Max_activations'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
